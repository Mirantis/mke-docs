{"/docs/":{"data":{"":"The Mirantis Kubernetes Engine 4 documentation set is provided to help system administrators and DevOps professionals to deploy MKE 4, covering key concepts and functionalities.\nLike the system sofware it seeks to represent, the MKE 4 documentation is also a pre-release product, intended to evolve as MKE 4 evolves. As such, feedback on the comprehensiveness and quality of the content herein is both welcome and essential.\n¬†\nFile an issue for MKE 4 documentation "},"title":"MKE 4 Documentation"},"/docs/concepts/":{"data":{"concepts#Concepts":"ConceptsThis section delves into the foundational concepts that underpin the architecture and configuration of MKE. Understanding these concepts is important for effectively deploying, managing, and scaling applications within a Kubernetes environment.\nArchitecture Configurations Blueprints "},"title":"_index"},"/docs/concepts/architecture/":{"data":{"":"Mirantis Kubernetes Engine (MKE) 4 is an enterprise-grade, production-ready Kubernetes platform that is designed to be secure, scalable, and reliable.\nYou can manage the entire MKE cluster through the MKE configuration file. Refer to Configuration for details.","components#Components":"MKE 4 is built on top of k0s, a lightweight Kubernetes distribution. To learn more, refer to k0s documentation.\nNetworking By default, Calico is installed as the Container Network Interface (CNI) plugin, with the following configuration:\nIPv4 only, with a fixed Pod CIDR of 10.244.0.0/16. The datastore mode set to kdd. kube-proxy set to iptables mode. A vxlan backend, which uses the default port of 4789 for traffic and default virtual network ID of 4096. Refer to Container Network Interface for the network configuration details"},"title":"Architecture"},"/docs/concepts/blueprints/":{"data":{"blueprints#Blueprints":"BlueprintsAll MKE 4 configuration files are translated into blueprints, a file type that is used to create Kubernetes Custom Resources (CRDs) that are also called blueprints. MKE 4 uses Blueprint Operator to manage blueprints and their assignments.\nBlueprint files must be in Kubernetes YAML format, and they must contain many of the same fields as a standard Kubernetes Object.\nA blueprint comprises three sections:\nKubernetes Provider Details the settings for the provider. For the most part, the Kubernetes Provider section is is managed by mkectl, independently of the user's MKE configuration file. Infrastructure Provides details that are used for the Kubernetes cluster; the hosts section of the MKE configuration file. Components Composed of addons that are specified in the MKE configuration file. The mkectl command transforms the configuration options into specific settings for either Helm or Manifest type addons that are deployed into the cluster. To view a detailed blueprint of an MKE configuration, run the mkectl init --blueprint command.\nüö´ It is possible to directly modify a blueprint. However, such modifications are considered advanced and support by MKE 4 is not assured. See the Blueprint Operator documentation for more details on blueprints."},"title":"blueprints"},"/docs/concepts/cni/":{"data":{"":"MKE supports Calico open-source as a Container Network Interface (CNI) plugin to enable the networking functionalities needed for container communication and management within a cluster.\n‚ö†Ô∏è Calico configuration is not migrated during the MKE 3 to 4 upgrade. ","configuration-example#Configuration example":"The network section of the MKE configuration file renders as follows:\nnetwork: serviceCIDR: 10.96.0.0/16 nodePortRange: 32768-35535 kubeProxy: disabled: false mode: iptables metricsbindaddress: 0.0.0.0:10249 iptables: masqueradebit: null masqueradeall: false localhostnodeports: null syncperiod: duration: 0s minsyncperiod: duration: 0s ipvs: syncperiod: duration: 0s minsyncperiod: duration: 0s scheduler: \"\" excludecidrs: [] strictarp: false tcptimeout: duration: 0s tcpfintimeout: duration: 0s udptimeout: duration: 0s nodeportaddresses: [] nllb: disabled: true cplb: disabled: true providers: - provider: calico enabled: true CALICO_DISABLE_FILE_LOGGING: true CALICO_STARTUP_LOGLEVEL: DEBUG FELIX_LOGSEVERITYSCREEN: DEBUG clusterCIDRIPv4: 192.168.0.0/16 deployWithOperator: false enableWireguard: false ipAutodetectionMethod: null mode: vxlan overlay: Always vxlanPort: 4789 vxlanVNI: 10000 - provider: kuberouter enabled: false deployWithOperator: false - provider: custom enabled: false deployWithOperator: false ","limitations#Limitations":"Components using nodeports may have their own specific way of specifying the port numbers for NodePorts, and these may need to be changed simultaneusly with the nodePortRange.","network-configuration#Network configuration":"The following table includes details on all of the configurable network fields.\nField Description Values Default serviceCIDR Sets the IPv4 range of IP addresses for services in a Kubernetes cluster. Valid IPv4 CIDR 10.96.0.0/16 nodePortRange Sets the allowed port range for Kubernetes services of the NodePort type. Valid port range 32768-35535 providers Sets the provider for the active CNI. calico calico ","providers-configuration#Providers configuration":"The following table includes details on the configurable settings for the Calico provider.\nField Description Values Default enabled Sets the name of the external storage provider. AWS is currently the only available option. true true clusterCIDRIPv4 Sets the IP pool in the Kubernetes cluster from which Pods are allocated. Valid IPv4 CIDR 192.168.0.0/16 ipAutodetectionMethod Sets the autodetecting method for the IPv4 address for the host. Provider specific1 None mode Sets the IPv4 overlay networking mode. ipip, vxlan vxlan vxlanPort Sets the VXLAN port for the VXLAN mode. Valid port number 4789 vxlanVNI Sets the VXLAN VNI for the VXLAN mode. Valid VNI number 10000 CALICO_STARTUP_LOGLEVEL Sets the early log level for calico/node. Provider specific1 DEBUG FELIX_LOGSEVERITYSCREEN Sets the log level for calico/felix. Provider specific1 DEBUG For the available values, consult your provider documentation.¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é"},"title":"Container Network Interface"},"/docs/concepts/configuration/":{"data":{"":"The Mirantis Kubernetes Engine (MKE) 4 configuration file contains an opinionated configuration on how to set up an MKE cluster.\nWith the MKE configuration file, you can:\nDefine the number of nodes in the cluster. Define ways to access the nodes. Enable or disable certain MKE components. Configure MKE component features Once set, the MKE configuration file is translated into a more complex blueprint that contains the granular details on how to set up the cluster.","choose-addons#Choose addons":"A core component of MKE 4 is a default set of curated and tested addons that you can install by running mkectl init and subsequently applying the generated configuration file.\nUsing the MKE configuration file, you can enable and disable the MKE 4 addons, as well as modify their settings. In addition, you can use the mkectl init command with the --blueprint option to print the generated blueprint that reflects your current MKE 4 configuration.","create-configuration#Create configuration":" Generate the default MKE configuration file by running:\nmkectl init \u003e mke.yaml Modify the hosts section of the MKE configuration file, to apply the configuration to a set of pre-existing machines that you have set up in advance:\nhosts: - ssh: address: 18.224.23.158 keyPath: \"/absolute/path/to/private/key.pem\" port: 22 user: root role: controller+worker - ssh: address: 18.224.23.158 keyPath: \"/absolute/path/to/private/key.pem\" port: 22 user: ubuntu role: worker - ssh: address: 18.117.87.45 keyPath: \"/absolute/path/to/private/key.pem\" port: 22 user: ubuntu role: worker "},"title":"Configuration"},"/docs/getting-started/":{"data":{"#":"Getting startedThis section contains easy and lightweight instructions on how to get started with your MKE installation.\nStep 1: Meet the system requirements Ensure that your system meets the necessary hardware and software requirements for installing and running an MKE cluster\nStep 2: Install the MKE CLI and other dependencies Install the MKE Command Line Interface (CLI), which is essential for managing your MKE installation. The CLI provides a convenient and powerful way to interact with your cluster, perform administrative tasks, and automate workflows.\nStep 3: Create a cluster Create your MKE cluster. This involves configuring the cluster settings, specifying the number of nodes, and setting up networking and storage options. Use the MKE CLI to initialize and configure the cluster according to your requirements.\nOnce the cluster is created, you can start deploying applications and services, leveraging the full capabilities of MKE for orchestration and management.","getting-started#Getting started":""},"title":"_index"},"/docs/getting-started/create-cluster/":{"data":{"":"","configure-cluster-nodes#Configure cluster nodes":"Configure the cluster nodes in advance, in accordance with the System requirements.\nNode provisioning is managed by the cluster administrators. You can, for instance, use Terraform to create the nodes in a cloud provider. Refer to Example Terraform configuration for an example.","create-a-cluster#Create a cluster":" Verify that there are no existing MKE clusters. You must not attempt to create a new cluster until you have first deleted the existing cluster. If you do make such an attempt, even through the use of a different configuration file, you will permanently lose access to the first cluster through mkectl.\nFor information on how to delete a cluster, refer to Uninstall a cluster.\nCreate a new cluster using mkectl apply command with the generated YAML configuration file:\nmkectl apply -f mke.yaml ‚ö†Ô∏è The mkectl apply command configures the mke context in the default kubeconfig file located at ~/.kube/config. If the default kubeconfig is changed, and the mke context becomes invalid or unavailable, mkectl will not manage the cluster until the kubeconfig is restored. Now, you can start interacting with the newly created cluster using kubectl with the mke context.","initialize-deployment#Initialize deployment":"MKE streamlines the cluster deployment through the use of a single YAML file, which details the desired cluster configuration. This approach simplifies the setup process and ensures consistency in cluster deployments.\nExample: A ready-to-deploy MKE configuration file hosts: - ssh: address: 1.1.1.1 # external IP of the first node keyPath: /path/to/ssh/key.pem port: 22 user: username role: controller+worker - ssh: address: 2.2.2.2 # external IP of the second node keyPath: /path/to/ssh/key.pem port: 22 user: username role: worker hardening: enabled: true authentication: enabled: true saml: enabled: false oidc: enabled: false ldap: enabled: false backup: enabled: true storage_provider: type: InCluster in_cluster_options: exposed: true tracking: enabled: true trust: enabled: true logging: enabled: true audit: enabled: true license: refresh: true apiServer: externalAddress: mke.example.com sans: [] ingressController: enabled: true replicaCount: 2 extraArgs: httpPort: 80 httpsPort: 443 enableSslPassthrough: false defaultSslCertificate: mke/auth-https.tls monitoring: enableGrafana: true enableOpscare: false network: kubeProxy: disabled: false mode: iptables metricsbindaddress: 0.0.0.0:10249 iptables: masqueradebit: null masqueradeall: false localhostnodeports: null syncperiod: duration: 0s minsyncperiod: duration: 0s ipvs: syncperiod: duration: 0s minsyncperiod: duration: 0s scheduler: \"\" excludecidrs: [] strictarp: false tcptimeout: duration: 0s tcpfintimeout: duration: 0s udptimeout: duration: 0s nodeportaddresses: [] nllb: disabled: true cplb: disabled: true providers: - provider: calico enabled: true CALICO_DISABLE_FILE_LOGGING: true CALICO_STARTUP_LOGLEVEL: DEBUG FELIX_LOGSEVERITYSCREEN: DEBUG clusterCIDRIPv4: 192.168.0.0/16 deployWithOperator: false enableWireguard: false ipAutodetectionMethod: null mode: vxlan overlay: Always vxlanPort: 4789 vxlanVNI: 10000 windowsNodes: false - provider: kuberouter enabled: false deployWithOperator: false - provider: custom enabled: false deployWithOperator: false Generate the YAML file for your installation:\nmkectl init \u003e mke.yaml In the generated configuration file:\nEdit the hosts section to match your roster of nodes. Provide the SSH information for each cluster node, as well as the role of the node based on their functions within the cluster. The table below provides the list of available node roles and their descriptions:\nNode Role Description controller+worker A manager node that runs both control plane and data plane components. This role combines the responsibilities of managing cluster operations and executing workloads. worker A worker node that runs the data plane components. These nodes are dedicated to executing workloads and handling the operational tasks assigned by the control plane. single A special role used when the cluster consists of a single node. This node handles both control plane and data plane components, effectively managing and executing workloads within a standalone environment. Specify the external address in the in apiServer.externalAddress field. The external address is the domain name of the load balancer configured as described in System Requirements: Load balancer.","install-dependecies#Install dependecies":"Verify that you have installed mkectl and other dependencies on your system as described in Install MKE CLI."},"title":"Create a cluster"},"/docs/getting-started/install-mke-cli/":{"data":{"":"Before you can proceed with the MKE installation, you need to download and install mkectl, the MKE CLI tool, along with the kubectl and k0sctl tools, either manually or using the script provided below.","install-manually#Install manually":"You can download mkectl from the S3 bucket:\nDistribution Architecture Download Linux arm64 download Linux x86_64 download MacOS arm64 download MacOS x86_64 download Windows arm64 download Windows x86_64 download Envisioned as a single binary, capable of managing MKE clusters without any additional dependencies, the MKE CLI requires that you have the following tools installed on your system as well:\nTool Version Download kubectl 1.29.0 or later download k0sctl 0.17.0 or later but less than 0.18.0 download ","install-using-script#Install using script":"To automatically install the necessary dependencies, you can utilize the install.sh script as shown in the example below.\nProcedure:\nInstall the dependencies by downloading and executing the following shell script:\nsudo /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Mirantis/mke-docs/main/content/getting-started/install.sh)\" If you want to override default dependency versions, pass the MKECTL_VERSION, KUBECTL_VERSION and K0SCTL_VERSION as required. For example:\nsudo K0SCTL_VERSION=0.17.4 /bin/sh -c \"$(curl -fsSL https://raw.githubusercontent.com/Mirantis/mke-docs/main/content/getting-started/install.sh)\" If you prefer to run the script in the debug mode for more detailed output and logging, set DEBUG=true:\nsudo DEBUG=true /bin/sh -c \"$(curl -fsSL https://raw.githubusercontent.com/Mirantis/mke-docs/main/content/getting-started/install.sh)\" Confirm the installations:\nmkectlk0sctlkubectl mkectl version Expected output:\nVersion: v4.0.0-alpha.2.0 k0sctl version Expected output:\nversion: v0.17.8 commit: b061291 If you passed the K0SCTL_VERSION=0.17.4 as illustrated above, the example output would be:\nversion: v0.17.4 commit: 372a589 kubectl version Expected output:\nClient Version: v1.30.0 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 Server Version: v1.29.3+k0s By default, the script installs the following software:\nTool Default version mkectl v4.0.0-alpha.2.0 k0sctl 0.17.8 kubectl v1.30.0 The install.sh script detects whether kubectl is already installed on your system and will not overwrite it. It also detects the operating system and the underlying architecture, based on which it will install the k0sctl, kubectl and mkectl binaries in /usr/local/bin. Thus, you must ensure that /usr/local/bin is in your PATH environment variable.\nNow, you can proceed with the MKE cluster creation."},"title":"Install the MKE CLI"},"/docs/getting-started/start-interacting-with-cluster/":{"data":{"":"To start interacting with the cluster, use kubectl with the mke context. Though, to do that you need to specify the configuration. Use mkectl to output the kubeconfig of the cluster to ~/mke/.mke.kubeconfig.\nYou can apply .mke.kubeconfig using any one of the following methods:\nSet the KUBECONFIG environment variable to point to ~/.mke/mke.kubeconfig:\nexport KUBECONFIG=~/.mke/\u003ccluster name\u003e.kubeconfig Append the contents to the default kubeconfig:\ncat ~/.mke/mke.kubeconfig \u003e\u003e ~/.kube/config Specify the kubeconfig as a command argument:\nkubectl --kubeconfig ~/.mke/mke.kubeconfig Example output:\n$ kubectl --context mke get nodes NAME STATUS ROLES AGE VERSION node1 Ready \u003cnone\u003e 2d v1.29.3+k0s node2 Ready control-plane 2d v1.29.3+k0s To modify the cluster configuration, edit the YAML configuration file and rerun the apply command:\nmkectl apply -f mke.yaml "},"title":"Start interacting with the cluster"},"/docs/getting-started/system-requirements/":{"data":{"":"Before you start cluster deployment, verify that your system meets the following minimum hardware\nand software requirements.","hardware-requirements#Hardware requirements":"MKE uses k0s as the underlying Kubernetes distribution. To learn the k0s hardware requirements, refer to the k0s documentation.","load-balancer-requirements#Load balancer requirements":"The load balancer can be implemented in many different ways. You can use for example HAProxy, NGINX, or the load balancer of your cloud provider.\nTo ensure the MKE Dashboard functions properly, MKE requires a TCP load balancer. This load balancer acts as a single point of contact to access the controllers. With the default MKE configuration, the load balancer must allow and route traffic to each controller through the following ports:\nListen port Target port Purpose Configurable 6443 6443 Kubernetes API 8132 8132 Konnectivity 9443 9443 Controller join API 443 33001 Ingress Controller You can configure the listen port of the Ingress Controller to be different from the default port 443. However, if you change the listen port, you must append the new port number to the external address in the configuration file. For example, if you set the listen port to be the same as the target port, 33001, the configuration should look as follows:\napiServer: externalAddress: \"mke.example.com:33001\" The target port must match the HTTPS node port of the Ingress Controller, which is 33001 by default, but can be adjusted as needed. See the configuration details for nodePorts in the Ingress Controller configuration","software-requirements#Software requirements":" Operating systems: Ubuntu 22.04 Linux Ubuntu 20.04 Linux Architecture: amd64 CNI: Calico "},"title":"System requirements"},"/docs/getting-started/uninstall-cluster/":{"data":{"":"Run the reset command to destroy the MKE cluster that was previously created with the apply command:\nmkectl reset -f mke.yaml You will be prompted to confirm the deletion of the cluster. To skip this step, use the --force flag with the reset command."},"title":"Uninstall a cluster"},"/docs/migrate-from-mke-3/":{"data":{"":"This section instructs you on how to migrate your existing MKE 3.7 cluster to the MKE 4.x version.","migrate-configuration#Migrate configuration":"In migrating to MKE 4 from MKE 3, you can directly transfer settings using mkectl.\nTo convert a local MKE 3 configuration for MKE 4: set the --mke3-config flag to convert a downloaded MKE 3 configuration file into a valid MKE 4 configuration file:\nmkectl init --mke3-config \u003c/path/to/mke3-config.toml\u003e ‚ÑπÔ∏è If you are upgrading an MKE3 cluster with GPU enabled, you must perform the GPU prerequisites before upgrading ","perform-the-migration#Perform the migration":"An upgrade from MKE 3 to MKE 4 consists of the following steps, all of which are performed through the use of the mkectl tool:\nRun pre-upgrade checks to verify the upgradability of the cluster. Carry out pre-upgrade migrations to prepare the cluster for a migration from a hyperkube-based MKE 3 cluster to a k0s-based MKE 4 cluster. Migrate manager nodes to k0s. Migrate worker nodes to k0s. Carry out post-upgrade cleanup, to remove MKE 3 components. Output the new MKE 4 config file. To upgrade an MKE 3 cluster, use the mkectl upgrade command:\nmkectl upgrade --hosts-path \u003cpath-to-hosts-yaml\u003e \\ --mke3-admin-username \u003cadmin-username\u003e \\ --mke3-admin-password \u003cadmin-password\u003e \\ --external-address \u003cexternal-address\u003e\\ --config-out \u003cpath-to-desired-file-location\u003e The external address is the domain name of the load balancer. For details, see System requirements: Load balancer requirements.\nThe --config-out flag allows you to specify a path where the MKE 4 configuration file will be automatically created and saved during migration. If not specified, the configuration file prints to your console on completion. In this case, save the output to a file for future reference\nThe upgrade process requires time to complete. Once the process is complete, run the following command to verify that the MKE 4 cluster is operating:\nsudo k0s kc get nodes Example output:\nNAME STATUS ROLES AGE VERSION ip-172-31-103-202.us-west-2.compute.internal Ready master 29m v1.29.3+k0s ip-172-31-104-233.us-west-2.compute.internal Ready master 29m v1.29.3+k0s ip-172-31-191-216.us-west-2.compute.internal Ready \u003cnone\u003e 29m v1.29.3+k0s ip-172-31-199-207.us-west-2.compute.internal Ready master 30m v1.29.3+k0s ‚ÑπÔ∏è The MKE 3 cluster will no longer be accessible through the previously created client bundle. The docker swarm cluster will no longer be accessible as well. In the event of an upgrade failure, the upgrade process rolls back, restoring the MKE 3 cluster to its original state.","prerequisites#Prerequisites":"Verify that you have the following components in place before you begin upgrading MKE3 to MKE 4:\nA running MKE 3.7.x cluster:\nkubectl get nodes NAME STATUS ROLES AGE VERSION ip-172-31-103-202.us-west-2.compute.internal Ready master 7m3s v1.27.7-mirantis-1 ip-172-31-104-233.us-west-2.compute.internal Ready master 7m3s v1.27.7-mirantis-1 ip-172-31-191-216.us-west-2.compute.internal Ready \u003cnone\u003e 6m59s v1.27.7-mirantis-1 ip-172-31-199-207.us-west-2.compute.internal Ready master 8m4s v1.27.7-mirantis-1 The latest mkectl binary, installed on your local enviroment:\nmkectl version Example output:\nVersion: v4.0.0-alpha.1.0 k0sctl version 0.17.4, installed on your local enviroment:\nk0sctl version Example output:\nversion: v0.17.4 commit: 372a589 A hosts.yaml file, to provide the information required by mkectl to connect to each node with SSH.\nExample hosts.yaml file:\ncat hosts.yaml hosts: - address: \u003chost1-external-ip\u003e port: \u003cssh-port\u003e user: \u003cssh-user\u003e keyPath: \u003cpath-to-ssh-key\u003e - address: \u003chost2-external-ip\u003e port: \u003cssh-port\u003e user: \u003cssh-user\u003e keyPath: \u003cpath-to-ssh-key\u003e "},"title":"Migrate from MKE 3.x"},"/docs/operations/":{"data":{"operations#Operations":"Operations"},"title":"_index"},"/docs/operations/authentication/":{"data":{"":"Mirantis Kubernetes Engine (MKE) supports OpenID Connect (OIDC), Security Assertion Markup Language (SAML), and Lightweight Directory Access Protocol (LDAP) authentication methods.\nMKE uses Dex for authentication. If you want to use a different authentication component, disable the authentication in the MKE configuration file and add your preferred method.\n‚ö†Ô∏è Be aware that if you opt to use an authentication method other than Dex, you will need to undertake all tasks and responsibilities associated with configuring and maintaining that method. ","configuration#Configuration":"You can configure authentication for MKE through the authentication section of the MKE configuration file.\nAuthentication is enabled by default. However, the settings for each of the individual authentication methods are disabled. To enable a service, set its enabled configuration option to true. Doing so will install the authentication method of your choice on your cluster.\nauthentication: enabled: true ","prerequisites#Prerequisites":" Identity Provider (IdP): To set OIDC or SAML you need to configure an IdP with an application. Refer to OIDC or SAML for detailed procedures.\nLDAP Server: To set LDAP, configure the LDAP server with the users as described in LDAP."},"title":"Authentication"},"/docs/operations/authentication/ldap/":{"data":{"":"You can configure LDAP (Lightweight Directory Access Protocol) for MKE 4 through the authentication section of the MKE configuration file. To enable the service, set enabled to true. The remaining fields in the authentication.ldap section are used to configure the interactions with your LDAP server. For more information, refer to the official DEX documentation LDAP configuration.","configure-mke#Configure MKE":"The MKE configuration file authentication.ldap fields are detailed below:\nField Description host Host and optional port of the LDAP server, in the host:port format. rootCA Path to a trusted root certificate file. bindDN Distinguished Name (DN) for an application service account. bindPW Password for an application service account. usernamePrompt Attribute to display in the password prompt. userSearch Settings to map user-entered username and password to an LDAP entry. userSearch.baseDN BaseDN from which to start the search. userSearch.filter Optional filter to apply for a user search of the directory. userSearch.username Username attribute to use for user entry comparison. userSearch.idAttr String representation of the user. userSearch.emailAttr Attribute to map to email. userSearch.nameAttr Attribute to map to display name of a user. userSearch.preferredUsernameAttr Attribute to map to preferred usernames. groupSearch Group search queries for groups given a user entry. groupSearch.baseDN BaseDN from which to start the search. groupSearch.filter Optional filter to apply for a group search of the directory. groupSearch.userMatchers Field pairs list to use to match a user to a group. groupSearch.nameAttr Group name. LDAP example configuration:\nauthentication: enabled: true ldap: enabled: true host: ldap.example.org:389 insecureNoSSL: true bindDN: cn=admin,dc=example,dc=org bindPW: admin usernamePrompt: Email Address userSearch: baseDN: ou=People,dc=example,dc=org filter: \"(objectClass=person)\" username: mail idAttr: DN emailAttr: mail nameAttr: cn ","test-authentication-flow#Test authentication flow":" ‚ÑπÔ∏è To test authentication flow, ports 5556 (dex) and 5555 (example-app) must be externally available. Navigate to http://{MKE hostname}:5555/login. Click Login to display the login page. Select Log in with LDAP. Enter the username and password for the LDAP server. Click Login. If authentication is successful, you will be redirected to the client applications home page. "},"title":"LDAP"},"/docs/operations/authentication/oidc-providers/oidc-okta-configuration/":{"data":{"":" Select OIDC - OpenID Connect for Sign-in method. Select Web Application for Application Type. For App integration name, choose a name that you can easily remember. Configure the host for your redirect URLs: Sign-in redirect URIs: http://{MKE hostname}/login Sign-out redirect URIs: http://{MKE hostname} Click Save to generate the clientSecret and clientID in the General table of the application. Add the generated clientSecret and clientID values to your MKE configuration file. Run the mkectl apply command with your MKE configuration file. "},"title":"Create OIDC application in Okta"},"/docs/operations/authentication/oidc-providers/oidc/":{"data":{"":"You can configure OIDC (OpenID Connect) for MKE 4 through the authentication section of the MKE configuration file. To enable the service, set enabled to true. The remaining fields in the authentication.oidc section are used to configure the OIDC provider.\nFor information on how to obtain the field values, refer to your chosen provider:\nOkta ","configure-mke#Configure MKE":"The MKE configuration file authentication.oidc fields are detailed below:\nField Description issuer OIDC provider root URL. clientID ID from the IdP application configuration. clientSecret Secret from the IdP application configuration. redirect URI URI to which the provider will return successful authentications. OIDC example configuration:\nauthentication: enabled: true oidc: enabled: true issuer: https://dev-94406016.okta.com clientID: 0oedtjcjrjWab3zlD5d4 clientSecret: DFA9NYLfE1QxwCSFkZunssh2HCx16kDl41k9tIBtFZaNcqyEGle8yZPtMBesyomD redirectURI: http://dex.example.com:32000/callback ","test-authentication-flow#Test authentication flow":" ‚ÑπÔ∏è To test authentication flow, ports 5556 (dex) and 5555 (example-app) must be externally available. Navigate to http://{MKE hostname}:5555/login Click Login to display the login page. Select Log in with OIDC. Enter your credentials and click Sign In. If authentication is successful, you will be redirected to the client applications home page. "},"title":"OIDC"},"/docs/operations/authentication/saml-providers/saml-okta-configuration/":{"data":{"":" Select SAML 2.0 for Sign-in method.\nFor App name, choose a name that you can easily remember.\nConfigure the host for your redirect URLs:\nSingle sign-on URL: http://{MKE hostname}/callback Audience URI (SP Entity ID): http://{MKE hostname}/callback Attribute statements: Name: email Value: user.email Name: name Value: user.login Click Save.\nClick Finish.\nNavigate to the Assignments tab:\na. Click Assign -\u003e Assign to people.\nb. Click the blue Assign button that corresponds to the account you want to use for authentication.\nOkta generates the ssoURL and certificate under the Sign On tab. The ssoURL is the MetadataURL with the final metadata removed from the path.\nDownload the certificate to the system from which you will run mkectl:\na. Navigate to the SAML Signing Certificates section.\nb. Click Actions for the active certificate and initiate the download.\nRun the mkectl apply command with your MKE configuration file."},"title":"Create SAML application in Okta"},"/docs/operations/authentication/saml-providers/saml/":{"data":{"":"You can configure SAML (Security Assertion Markup Language) for MKE 4 through the authentication section of the MKE configuration file. To enable the service, set enabled to true. The remaining fields in the authentication.saml section are used to configure the SAML provider. For information on how to obtain the field values, refer to your chosen provider:\nOkta For more information, refer to the official DEX documentation Authentication through SAML 2.0.","configure-mke#Configure MKE":"The MKE configuration file authentication.smal fields are detailed below:\nField Description enabled Enable authentication through dex. ssoMetadataURL Metadata URL provided by some IdPs, with which MKE can retrieve information for all other SAML configurations. ca Certificate Authority (CA) alternative to caData to use when validating the signature of the SAML response. Must be manually mounted in a local accessible by dex. caData CA alternative to ca, which you can use to place the certificate data directly into the config file. ssoURL URL to provide to users to sign into MKE 4 with SAML. Provided by the IdP. redirectURI Callback URL for dex to which users are returned to following successful IdP authentication. insecureSkipSignatureValidation Optional. Use to skip the signature validation. For testing purposes only. usernameAttr Username attribute in the returned assertions, to map to ID token claims. emailAttr Email attribute in the returned assertions, to map to ID token claims. groupsAttr Optional. Groups attribute in the returned assertions, to map to ID token claims. entityIssuer Optional. Include as the Issuer value during authentication requests. ssoIssuer Optional. Issuer value that is expected in the SAML response. groupsDelim Optional. If groups are assumed to be represented as a single attribute, this delimiter splits the attribute value into multiple groups. nameIDPolicyFormat Requested name ID format. An example configuration for SAML:\nauthentication: enabled: true saml: enabled: true ssoURL: https://dev64105006.okta.com/app/dev64105006_mke4saml_1/epkdtszgindywD6mF5s7/sso/saml redirectURI: http://{MKE host}:5556/callback usernameAttr: name emailAttr: email ","test-authentication-flow#Test authentication flow":" ‚ÑπÔ∏è To test authentication flow, ports 5556 (dex) and 5555 (example-app) must be externally available. Navigate to http://{MKE hostname}:5555/login. Click Login to display the login page. Select Log in with SAML. Enter your credentials and click Sign In. If authentication is successful, you will be redirected to the client applications home page. "},"title":"SAML"},"/docs/operations/backup-restore/":{"data":{"":"MKE 4 supports backup and restore of cluster data through the use of the Velero addon. Backup is enabled by default.","configuration#Configuration":"The backup section of the MKE4 configuration file renders as follows:\nbackup: enabled: true storage_provider: type: InCluster in_cluster_options: exposed: true distributed: false By default, MKE 4 supports backups that use the in-cluster storage provider, as indicated in the type option setting of InCluster. MKE 4 in-cluster backups are implemented using the MinIO addon.\nThe exposed option setting of true indicates that the MinIO service is exposed through NodePort, which Velero requires to function correctly. Core backup functionality should work, however, even if the service is not exposed.\nThe distributed option configures MinIO storage to run in distributed mode.\nRefer to the following table for detail on all of the conifguration file backup fields:\nField Description Valid values Default enabled Indicates whether backup/restore functionality is enabled. true, false true storage_provider.type Indicates whether the storage type in use is in-cluster or external. InCluster, External InCluster storage_provider.in_cluster_options.exposed Indicates whether to expose InCluster (MinIO) storage through NodePort. true, false true storage_provider.in_cluster_options.distributed Indicates whether to run MinIO in distributed mode. true, false false storage_provider.external_options.provider Name of the external storage provider. AWS is currently the only available option. aws aws storage_provider.external_options.bucket Name of the pre-created bucket to use for backup storage. \"\" \"\" storage_provider.external_options.region Region in which the bucket exists. \"\" \"\" storage_provider.external_options.credentials_file_path Path to the credentials file. \"\" \"\" storage_provider.external_options.credentials_file_profile Profile in the Credentials file to use. \"\" \"\" ","create-backups-and-perform-restores#Create backups and perform restores":"For information on how to create backups and perform restores for both storage provider types, refer to:\nIn-cluster storage provider: in_cluster.md External storage provider: external.md ","existing-limitations#Existing limitations":" Scheduled backups, an MKE 3 feature that is planned for integration to MKE 4, have not yet been implemented.\nBackups must currently be restored in the same cluster in which the backup was taken, and thus restoring a backup to a new set of nodes is not yet supported for the in-cluster storage provider."},"title":"Back up and restore"},"/docs/operations/backup-restore/external/":{"data":{"":"You can configure MKE 4 to store backups and restores externally, for example in object storage provided by a public cloud provider.\n‚ÑπÔ∏è AWS S3 is currently the only supported external backup store that MKE 4 supports. ","configuration#Configuration":" Copy the credentials information from the AWS console to create an IAM credentials file.\nEdit the storage_provider section of the MKE configuration file to point to the file, including the profile name.\nCreate an S3 bucket and point the configuration to the bucket and region.\nExample configuration:\nstorage_provider: type: External external_options: provider: aws bucket: bucket_name region: us-west-2 credentials_file_path: \"/path/to/iamcredentials\" credentials_file_profile: \"386383511305_docker-testing\" Once you have configured the AWS backup storage and the MKE configuration file has been applied, verify the existence of the BackupStorageLocation custom resource.\nkubectl get backupstoragelocation -n mke After you run mkectl apply the output may require a few minutes to display.\nExample output:\nNAME PHASE LAST VALIDATED AGE DEFAULT default Available 20s 32s true ","create-backups-and-perform-restores#Create backups and perform restores":"With configuration complete, you can now create backups and perform restores from those backups. After you have run a restore operation from a backup, the Kubernetes cluster state should resemble what it was at the time you created that backup.\nmkectl backup create --name aws-backup Example output:\nINFO[0000] Creating backup aws-backup... Backup request \"aws-backup\" submitted successfully. Run `velero backup describe aws-backup` or `velero backup logs aws-backup` for more details. INFO[0000] Waiting for backup aws-backup to complete... INFO[0003] Waiting for backup to complete. Current phase: InProgress INFO[0006] Waiting for backup to complete. Current phase: InProgress INFO[0009] Waiting for backup to complete. Current phase: InProgress INFO[0012] Waiting for backup to complete. Current phase: InProgress INFO[0015] Waiting for backup to complete. Current phase: Completed INFO[0015] Backup aws-backup completed successfully To list the backups, run the mkectl backup list command:\nmkectl backup list Example output:\nNAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR aws-backup Completed 0 0 2024-05-08 16:17:18 -0400 EDT 29d default \u003cnone\u003e To perform a restore using an external backup, run:\nmkectl restore create --name aws-backup Example output:\nINFO[0000] Waiting for restore aws-backup-20240508161811 to complete... INFO[0000] Waiting for restore to complete. Current phase: InProgress INFO[0003] Waiting for restore to complete. Current phase: InProgress INFO[0006] Waiting for restore to complete. Current phase: InProgress INFO[0009] Waiting for restore to complete. Current phase: InProgress INFO[0012] Waiting for restore to complete. Current phase: InProgress INFO[0015] Waiting for restore to complete. Current phase: InProgress INFO[0018] Waiting for restore to complete. Current phase: InProgress INFO[0021] Waiting for restore to complete. Current phase: InProgress INFO[0024] Waiting for restore to complete. Current phase: Completed INFO[0024] Restore aws-backup-20240508161811 completed successfully To list the restores, run:\nmkectl restore list Example output:\nNAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR aws-backup-20240508161811 aws-backup Completed 2024-05-08 16:18:11 -0400 EDT 2024-05-08 16:18:34 -0400 EDT 0 108 2024-05-08 16:18:11 -0400 EDT \u003cnone\u003e From your AWS console, you can see that both the backup and restore are created in the S3 bucket:"},"title":"Use an external storage provider"},"/docs/operations/backup-restore/in-cluster/":{"data":{"":"By default, MKE 4 stores backups and restores using the in-cluster storage provider, the MinIO addon.\nThe offered instructions assume that you have created a cluster and applied a blueprint with the default MKE backup configuration.","create-an-in-cluster-backup#Create an in-cluster backup":"To create an in-cluster backup, run:\nmkectl backup create --name \u003cname\u003e Example output:\nmkectl backup create --name test INFO[0000] Creating backup test... Backup request \"test\" submitted successfully. Run `velero backup describe test` or `velero backup logs test` for more details. INFO[0000] Waiting for backup test to complete... INFO[0003] Waiting for backup to complete. Current phase: InProgress INFO[0006] Waiting for backup to complete. Current phase: InProgress INFO[0009] Waiting for backup to complete. Current phase: InProgress INFO[0012] Waiting for backup to complete. Current phase: InProgress INFO[0015] Waiting for backup to complete. Current phase: Completed The backup should be present in the MinIO bucket. To list the backups, run the mkectl backup list command:\nExample output:\nmkectl backup list NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR test Completed 0 0 2024-05-07 17:29:18 -0400 EDT 29d default \u003cnone\u003e Optionally, you can view detailed logs of a backup by running the mkectl backup logs --name test command.","restore-from-an-in-cluster-backup#Restore from an in-cluster backup":"To perform a restore using an in-cluster backup, run:\nmkectl restore create --name test Example output:\nmkectl restore create --name test INFO[0000] Waiting for restore test-20240507173309 to complete... INFO[0000] Waiting for restore to complete. Current phase: InProgress INFO[0003] Waiting for restore to complete. Current phase: InProgress INFO[0006] Waiting for restore to complete. Current phase: InProgress INFO[0009] Waiting for restore to complete. Current phase: InProgress INFO[0012] Waiting for restore to complete. Current phase: InProgress INFO[0015] Waiting for restore to complete. Current phase: InProgress INFO[0018] Waiting for restore to complete. Current phase: InProgress INFO[0021] Waiting for restore to complete. Current phase: InProgress INFO[0024] Waiting for restore to complete. Current phase: InProgress INFO[0027] Waiting for restore to complete. Current phase: Completed INFO[0027] Restore test-20240507173309 completed successfully To list the restores, run:\nmkectl restore list Example output:\nmkectl restore list NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR test-20240507173309 test Completed 2024-05-07 17:33:09 -0400 EDT 2024-05-07 17:33:34 -0400 EDT 0 121 2024-05-07 17:33:09 -0400 EDT \u003cnone\u003e Optionally, you can view detailed logs by running the mkectl restore logs --name test-20240507173309 command."},"title":"Use the in-cluster storage provider"},"/docs/operations/dashboard/":{"data":{"":" ‚ÑπÔ∏è Available since 4.0.0-alpha.2.0 The MKE Dashboard add-on provides the web UI to manage Kubernetes resources management:\nTo install the Dashboard add-on:\nThe dashboard is enabled by default in the MKE installation."},"title":"MKE Dashboard"},"/docs/operations/gpu/":{"data":{"":"Mirantis Kubernetes Engine (MKE) supports running workloads on GPU nodes. Current support is limited to NVIDIA GPUs. MKE uses the NVIDIA GPU Operator to manage GPU resources on the cluster.\nTo enable GPU support, you must install the NVIDIA GPU Operator on your cluster and configure MKE to use it.","configuration#Configuration":"GPU support is disabled by default. To enable GPU support, you must configure the gpu section of the MKE configuration file.\ngpu: enabled: true This informs MKE to install the NVIDIA GPU Operator on your cluster.","prerequisites#Prerequisites":"Before you can enable GPU support in MKE, you must install the NVIDIA GPU toolkit on each GPU enabled node in your cluster.\nsudo nvidia-ctk runtime configure --runtime=containerd -config /etc/k0s/containerd.toml ","running-gpu-workloads#Running GPU Workloads":"This example runs a simple GPU workload that reports detected NVIDIA GPU devices.\nkubectl apply -f- \u003c\u003cEOF apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: run: gpu-test name: gpu-test spec: replicas: 1 selector: matchLabels: run: gpu-test template: metadata: labels: run: gpu-test spec: containers: - command: - sh - -c - \"deviceQuery \u0026\u0026 sleep infinity\" image: kshatrix/gpu-example:cuda-10.2 name: gpu-test resources: limits: nvidia.com/gpu: \"1\" EOF Verify that the deployment is in a running state\nkubectl get pods | grep \"gpu-test\" You should see output similar to the following:\nNAME READY STATUS RESTARTS AGE gpu-test-747d746885-hpv74 1/1 Running 0 14m Review the logs of the gpu-test pod. Result = PASS indicates a successful detection of the NVIDIA GPU device.\nkubectl logs gpu-test-747d746885-hpv74 You should see output similar to the following:\ndeviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \"Tesla V100-SXM2-16GB\" ... deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.2, CUDA Runtime Version = 10.2, NumDevs = 1 Result = PASS ","upgrading#Upgrading":"If you are upgrading from an MKE3 cluster with GPU enabled, the prerequisites\nmust be done before starting the upgrade process. Otherwise, the upgrade will\nsee that the GPU is enabled in the MKE3 configuration and will transfer that\nconfiguration to MKE4."},"title":"GPU Workloads"},"/docs/operations/ingress/":{"data":{"":"Traffic that originates outside of your cluster, ingress traffic, is managed through the use of an ingress controller. By default, MKE 4 offers NGINX Ingress Controller, which manages ingress traffic using the Kubernetes Ingress rules.\nNGINX Ingress Controller is the only one ingress controller that MKE 4 currently supports.","configuration#Configuration":"You can configure NGINX Ingress Controller through the ingressController section of the MKE 4 configuration file. The function is enabled by default and must not be disabled for the cluster to function correctly.\nIngress controller parameters that you can configure are detailed in the following table.\nField Description Default replicaCount Sets the number of NGINX Ingress Controller deployment replicas. 2 enableLoadBalancer Enables an external load balancer. Valid values: true, false. true if apiServer.externalAddress is set in the config file; false otherwise extraArgs Additional command line arguments to pass to Ingress-Nginx Controller. {} (empty) extraArgs.httpPort Sets the container port for servicing HTTP traffic. 80 extraArgs.httpsPort Sets the container port for servicing HTTPS traffic. 443 extraArgs.enableSslPassthrough Enables SSL passthrough. false extraArgs.defaultSslCertificate The only valid value is mke/auth-https.tls. Must NOT be changed. mke/auth-https.tls preserveClientIP Enables preserving inbound traffic source IP. Valid values: true, false. false externalIPs Sets the list of external IPs for Ingress service. IP addresses of managers nodes are always added automatically. [] affinity Sets node affinity. Example Affinity is always configured to schedule ingress controller pods on manager nodes. Additional rules may be added, but it‚Äôs not recommended. For more information, refer to the Kubernetes documentation Affinity and anti-affinity. {} (empty) tolerations Sets node toleration. Example Tolerations are always configured to allow scheduling on manager nodes. Additional rules may be added, but it‚Äôs not recommended. Refer to the Kubernetes documentation Assigning Pods to Nodes for more detail. [] (empty) configMap Adds custom configuration options to Nginx. For a complete list of available options, refer to the NGINX Ingress Controller ConfigMap. {} (empty) tcpServices Sets TCP service key-value pairs; enables TCP services. Example Refer to the NGINX Ingress documentation Exposing TCP and UDP services for more information. for more information. {} (empty) udpServices Sets UDP service key-value pairs; enables UDP services. Example Refer to the NGINX Ingress documentation Exposing TCP and UDP services for more information. {} (empty) nodePorts Sets the node ports for the external HTTP/HTTPS/TCP/UDP listener. You should not change the HTTPS port, but if you do so, make sure to change the target port of the MKE Dashboard in your load balancer configuration. Refer to System requirements for more information. HTTP: 33000, HTTPS: 33001 ports Sets the port for the internalHTTP/HTTPS listener. HTTP: 80, HTTPS: 443 disableHttp Disables the HTTP listener. false Affinity You can specify node affinities using the ingressController.affinity.nodeAffinity field in the MKE configuration file.\nThe following example uses requiredDuringSchedulingIgnoredDuringExecution to schedule the ingress controller pods.\ningressController: enabled: true affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - ip-172-31-42-30 Tolerations You can set Node tolerations for server scheduling to nodes with taints using the ingressController.tolerations field in the MKE configuration file.\nThe following example uses a toleration with NoExecute effect.\ningressController: enabled: true tolerations: - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoExecute\" ","example-ingress-controller-configuration#Example ingress controller configuration":" ingressController: enabled: true enableLoadBalancer: false numReplicas: 1 preserveClientIP: true tolerations: - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoExecute\" extraArgs: httpPort: 80 httpsPort: 443 enableSslPassthrough: false configMap: access-log-path: \"/var/log/nginx/access.log\" generate-request-id: \"true\" use-forwarded-headers: \"true\" error-log-path: \"/var/log/nginx/error.log\" tcpServices: 9000: \"default/tcp-echo:9000\" udpServices: 5005: \"default/udp-listener:5005\" nodePorts: http: 33000 https: 33001 tcp: 9000: 33011 udp: 5005: 33012 ports: http: 8080 https: 4443 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - ip-172-31-42-30 ","mke-version-comparison-ingress-configuration-parameters#MKE version comparison: Ingress configuration parameters":" MKE-3 MKE-4 [cluster_config.ingress_controller.enabled] ingressController.enabled [cluster_config.ingress_controller.ingress_num_replicas] ingressController.numReplicas [cluster_config.ingress_controller.ingress_enable_lb] ingressController.enableLoadBalancer [cluster_config.ingress_controller.ingress_preserve_client_ip] ingressController.preserveClientIP [[cluster_config.ingress_controller.ingress_node_toleration]] key = ‚Äúcom.docker.ucp.manager‚Äù value = \"\" operator = ‚ÄúExists‚Äù effect = ‚ÄúNoSchedule‚Äù ingressController.tolerations - key: ‚Äúkey1‚Äù operator: ‚ÄúEqual‚Äù value: ‚Äúvalue1‚Äù effect: ‚ÄúNoExecute‚Äù [cluster_config.ingress_controller.ingress_config_map] ingressController.configMap [cluster_config.ingress_controller.ingress_tcp_services] 9000 = ‚Äúdefault/tcp-echo:9000‚Äù ingressController.tcpServices:9000: ‚Äúdefault/tcp-echo:9000‚Äù [cluster_config.ingress_controller.ingress_udp_services] 5005 = ‚Äúdefault/udp-listener:5005‚Äù ingressController.udpServices: 5005: ‚Äúdefault/udp-listener:5005‚Äù [cluster_config.ingress_controller.ingress_extra_args] http_port = 8080 https_port = 4443 enable_ssl_passthrough = true default_ssl_certificate = \"\" ingressController.extraArgs: httpPort: 0 httpsPort: 0 enableSslPassthrough: true defaultSslCertificate: \"\" [cluster_config.ingress_controller.ingress_node_affinity] ingressController.affinity [[cluster_config.ingress_controller.ingress_exposed_ports]] name = ‚Äúhttp2‚Äù port = 80 target_port = 8080 node_port = 33001 protocol = \"\" Deprecated in MKE 4. The http and https ports are enabled by default on 80 and 443 respectively. If the user wants to change it, they can use ingressController.ports.\nNodePorts for http and https can be configured via ingressController.nodePorts. The default values are 33000 and 33001 respectively. For information on how to configure TCP/UDP ports, refer to the TCP and UDP services documentation. "},"title":"Ingress controller"},"/docs/operations/ingress/tcp-udp-services/":{"data":{"":"The Kubernetes ingress resource only supports services over HTTP and HTTPS.\nUsing NGINX Ingress Controller, however, you can receive external TCP/UDP\ntraffic from non-HTTP protocols and route them to internal services using\nTCP/UDP port mappings.","expose-a-tcp-service#Expose a TCP service":"To expose TCP services, set the following parameters in the MKE 4 configuration file.\nField Description ingressController.tcpServices Indicates TCP service key-value pairs. ingressController.nodePorts.tcp Sets node port mapping for external TCP listeners. In the example procedure, a tcp-echo service that is running in the default namespace on port 9000 is exposed using the port 9000, on NodePort 33011.\nDeploy a sample TCP service listening on port 9000, to echo back any text it receives with the prefix hello:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: tcp-echo labels: app: tcp-echo service: tcp-echo spec: selector: app: tcp-echo ports: - name: tcp port: 9000 --- apiVersion: apps/v1 kind: Deployment metadata: name: tcp-echo spec: replicas: 1 selector: matchLabels: app: tcp-echo template: metadata: labels: app: tcp-echo spec: containers: - name: tcp-echo image: docker.io/istio/tcp-echo-server:1.2 imagePullPolicy: IfNotPresent args: [ \"9000\", \"hello\" ] ports: - containerPort: 9000 EOF Verify that the deployment was created correctly:\nkubectl get deploy tcp-echo Example output:\nNAME READY UP-TO-DATE AVAILABLE AGE tcp-echo 1/1 1 1 39s Verify that the service is running:\nkubectl get service tcp-echo Example output:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE tcp-echo ClusterIP 10.96.172.90 \u003cnone\u003e 9000/TCP 46s Configure Ingress Controller to expose the TCP service:\nVerify that the enabled parameter for the ingressController option in the MKE configuration file is set to true.\nModify the MKE configuration file to expose the newly created TCP service:\ningressController: enabled: true tcpServices: \"9000\": default/tcp-echo:9000 nodePorts: tcp: 9000: 33011 Apply the MKE configuration file:\nmkectl apply -f mke.yaml Test the TCP service by sending the text hello world:\necho \"world\" | netcat \u003cWORKER_NODE_IP\u003e 33011 hello world The service should respond with hello world.\nCheck the tcp-echo logs:\nObtain the Pod name:\nkubectl get pods --selector=app=tcp-echo Example output:\nNAME READY STATUS RESTARTS AGE tcp-echo-544849bd8f-6jscx 1/1 Running 0 45h Access the log:\nkubectl logs tcp-echo-544849bd8f-6jscx Example output:\nlistening on [::]:9000, prefix: hello request: world response: hello world Remove the Kubernetes resources, which are no longer needed:\nkubectl delete service tcp-echo kubectl delete deployment tcp-echo ","expose-a-udp-service#Expose a UDP service":"To expose UDP services, set the following parameters in the MKE 4 configuration file.\nField Description ingressController.udpServices Indicates UDP service key-value pairs. ingressController.nodePorts.udp Sets node port mapping for external UDP listeners. In the example procedure, a udp-listener service running in the default namespace on port 5005 is exposed using the port 5005, on NodePort 33012.\nDeploy a sample UDP service listening on port 5005:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata: name: udp-listener --- apiVersion: apps/v1 kind: Deployment metadata: name: udp-listener labels: app: udp-listener spec: replicas: 1 selector: matchLabels: app: udp-listener template: metadata: labels: app: udp-listener spec: containers: - name: udp-listener image: mendhak/udp-listener ports: - containerPort: 5005 protocol: UDP name: udp --- apiVersion: v1 kind: Service metadata: name: udp-listener spec: ports: - port: 5005 targetPort: 5005 protocol: UDP name: udp selector: app: udp-listener EOF Verify that the deployment was created correctly:\nkubectl get deploy udp-listener Example output:\nNAME READY UP-TO-DATE AVAILABLE AGE udp-listener 1/1 1 1 31s Verify that the service is running:\nkubectl get service udp-listener Example output:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE udp-listener ClusterIP 10.96.19.229 \u003cnone\u003e 5005/UDP 37s Configure Ingress Controller to expose the UDP service:\nVerify that the enabled parameter for the ingressController option in the MKE configuration file is set to true.\nModify the MKE configuration file to expose the newly created UDP service:\ningressController: enabled: true udpServices: \"5005\": default/udp-listener:5005 nodePorts: udp: 5005: 33012 Apply the MKE configuration file:\nmkectl apply -f mke.yaml Test the UDP service by sending the UDP Datagram Message:\necho \"UDP Datagram Message\" | netcat -v -u \u003cWORKER_NODE_IP\u003e 33012 Check the udp-listener logs:\nObtain the Pod name:\nkubectl get pods --selector=app=udp-listener Example output:\nNAME READY STATUS RESTARTS AGE udp-listener-f768d8db4-v69jr 1/1 Running 0 44h Access the log:\nkubectl logs udp-listener-f768d8db4-v69jr Example output:\nListeningon UDP port 5005 UDP Datagram Message Remove the Kubernetes resources, which are no longer needed:\nkubectl delete service udp-listener kubectl delete deployment udp-listener "},"title":"TCP and UDP services"},"/docs/operations/monitoring/":{"data":{"":"The MKE 4 monitoring setup is based on the kube-prometheus-stack, offering a comprehensive solution for collecting, storing, and visualizing metrics.","grafana#Grafana":"Grafana is an open-source monitoring platform that provides a rich set of tools for visualizing time-series data. It includes a variety of graph types and dashboards.\nGrafana is enabled in MKE by default and may be disabled through the MKE configuration file:\nmonitoring: enableGrafana: true To access the Grafana dashboard:\nRun the following command to port-forward Grafana:\nkubectl --namespace mke port-forward svc/monitoring-grafana 3000:80 Go to http://localhost:3000.","monitoring-tools#Monitoring tools":"Detail for the MKE 4 monitor tools is provided in the following table:\nMonitoring tool Default state Configuration key Description Grafana enabled monitoring.enableGrafana Provides a web interface for viewing metrics and logs collected by Prometheus Prometheus enabled - Collects and stores metrics Opscare disabled monitoring.enableOpscare (Under development) Supplies additional monitoring capabilities, such as Alertmanager ","opscare-under-development#Opscare (Under development)":"Mirantis OpsCare is an advanced monitoring and alerting solution. Once it is integrated, Mirantis Opscare will enhance the monitoring capabilities of MKE 4 by incorporating additional tools and features, such as Prometheus Alertmanager.\nDisabled by default, you can enable Mirantis Opscare through the MKE configuration file.\nmonitoring: enableOpscare: true ","prometheus#Prometheus":"Prometheus is an open-source monitoring and alerting toolkit that is designed for reliability and scalability. It collects and stores metrics as time series data, providing powerful query capabilities and a flexible alerting system.\nTo access the Prometheus dashboard:\nRun the following command to port-forward Prometheus:\nkubectl --namespace mke port-forward svc/prometheus-operated 9090 Go to http://localhost:9090."},"title":"Monitoring"},"/docs/operations/support-bundle/":{"data":{"":"For the Alpha.1 release, MKE 4 deploys k0s v1.29.3, which does not inherently support the Replicated support bundle tool.","collect-host-information-using-the-k0s-provided-yaml-file#Collect host information using the k0s-provided YAML file":" Obtain the k0s-provided YAML file.\nRun the support-bundle tool:\n./support-bundle --kubeconfig /var/lib/k0s/pki/admin.conf \u003csupport-bundle-worker.yaml\u003e ‚ÑπÔ∏è The support-bundle tool requires that the kubeconfig file be passed as an argument. The kubeconfig file is located at /var/lib/k0s/pki/admin.conf. Now, you can find the support bundle with the collected host information at support-bundle-\u003ctimestamp\u003e.tar.gz.","manually-collect-a-support-bundle#Manually collect a support bundle":" SSH into the manager node.\nDownload and install the support-bundle tool:\ncurl -L https://github.com/replicatedhq/troubleshoot/releases/latest/download/support-bundle_linux_amd64.tar.gz | tar xzvf - Create a YAML file that details the support bundle configuration.\nExample support-bundle-worker.yaml file:\napiVersion: troubleshoot.sh/v1beta2 kind: SupportBundle metadata: name: sample spec: collectors: - logs: selector: - app.kubernetes.io/name=blueprint-webhook namespace: blueprint-system name: logs/blueprint-system - logs: selector: - control-plane=controller-manager namespace: blueprint-system name: logs/blueprint-system This configuration accomplishes the following:\nCaptures of cluster information Sets of cluster resources Collects logs from the blueprint-controller-manager and blueprint-operator-webhook pods, in the logs/ directory of the output. "},"title":"Support bundle"},"/docs/release-notes/":{"data":{"release-notes#Release notes":"Release notesWelcome to the release notes for our latest software update. This section provides a comprehensive overview of the new features, enhancements, bug fixes, and any known issues. Our goal is to keep you informed about the changes and improvements, ensuring you can make the most of the new capabilities and optimizations.\nFeatures Known issues "},"title":"_index"},"/docs/release-notes/features/":{"data":{"":"The table that follows details MKE 4 features and their current status. In addition, where applicable, the table offers links to associated documentation.\nFeature Pre-Release Status Authentication alpha.1 MVP Authorization alpha.1 MVP Backup and restore alpha.1 MVP CIS Benchmark CLI Cloud Providers CoreDNS cAdvisor gMSA GPU Feature Discovery Ingress alpha.1 MVP Kubernetes alpha.1 MVP 1.29 Life Cycle Management Licensing Load Balancing Logging, Monitoring and Alerting alpha.1 MVP Networking (CNI) alpha.2 MVP Node Feature Discovery Offline Bundle OpsCare Policy Controller Storage (CSI) Support Dump Telemetry TLS 2FA Web UI alpha.2.0 Windows ","400-alpha20#4.0.0-alpha.2.0":"MKE Dashboard Metrics including node performance with usage statistics Basic Kubernetes management (View, Create, Update, and Delete): Configurations: ConfigMaps Controllers: ReplicaSets, ReplicationControllers, StatefulSets, Jobs, CronJobs, Daemonsets, and Deployments Namespaces Nodes Pods Services ServiceAccounts Storage: StorageClasses, PersistantVolumes + PersistantVolumeClaims View with Kubernetes resources by namespace or all namespaces at once Namespace management Namespaces can now be managed by clicking on the active namespace in the side menu. This opens the namespace selector.\nFrom there, if you want to manage your namespaces further (add, remove, etc), you can click the Manage namespaces button at the bottom left to view the list of namespaces.\nTo return to the main menu, click the ‚ÄúDashboard‚Äù icon in the breadcrumbs or the back arrow next to the ‚Äúactive namespace‚Äù in the side navigation.\nSign out, MKE Version, and documentation links The MKE version is now indicated in a new menu at the top right of the dashboard. Here you will also find links to documentation and the ‚ÄúSign out‚Äù option.\nIn development User management User settings Admin settings: authentication, ingress, backups, certificates, telemetry, and logging Support bundle access App notifications and alerts Improved detail view of individual Kubernetes resources Statuses for Kubernetes objects ‚Ä¶ and more "},"title":"Features"},"/docs/release-notes/known-issues/":{"data":{"":"The MKE 4 known issues with available workarounds are described herein.","bop-583-ldap-settings-fail-to-migrate-during-upgrade-from-mke-3#[BOP-583] LDAP settings fail to migrate during upgrade from MKE 3":"LDAP configurations are not stored in MKE 3 configuration files, and thus they are not included when you upgrade to MKE 4 from an MKE 3 installation.\nWorkaround:\nWhen upgrading from MKE 3, you must manually add the LDAP configuration.\nMake a request to https://{{host}}/enzi/v0/config/auth/ldap on the MKE 3 cluster prior to the migration. For more information, refer to the MKE 3 LDAP Configuration through API documentation.\nConvert the LDAP response to the MKE 4 LDAP settings.\nApply the translated LDAP settings to the cluster following migration.","bop-708-oidc-authentication-fails-after-mkectl-upgrade#[BOP-708] OIDC authentication fails after mkectl upgrade":"Due to an issue with client secret migration, OIDC authentication fails following an upgrade performed with mkectl.\nWorkaround:\nCopy the MKE 4 configuration that prints at the end of migration.\nUpdate the authentication.oidc.clientSecret field to the secret field from your identity provider.\nApply the updated MKE 4 configuration.","bop-898bop-899-calico-ebpf-and-ipvs-modes-are-not-supported#[BOP-898][BOP-899] Calico eBPF and IPVS modes are not supported":"Calico eBPF and IPVS mode are not yet supported for MKE 4. As such, upgrading from an MKE 3 cluster using either of those networking modes results in an error:\nFATA[0640] Upgrade failed due to error: failed to run step [Upgrade Tasks]: unable to install BOP: unable to apply MKE4 config: failed to wait for pods: failed to wait for pods: failed to list pods: client rate limiter Wait returned an error: context deadline exceeded ","bop-905-prometheus-dashboard-reports-incorrect-heavy-memory-use#[BOP-905] Prometheus dashboard reports incorrect heavy memory use":"The Prometheus dashboard displays heavy memory use that does not accurately\nreflect true memory status.","bop-947-managed-user-passwords-are-not-migrated-during-upgrade-from-mke-3#[BOP-947] Managed user passwords are not migrated during upgrade from MKE 3":"The admin password is migrated during upgrade from MKE 3, however all other managed user passwords are not migrated.","bop-964-mke-operator-in-crashloopbackoff-status#[BOP-964] mke-operator in crashloopbackoff status":"The mke-operator-controller-manager is in crashloopbackoff status in MKE 4\nAlpha 2. You can safely ignore this, however, as it has no effect on MKE\n4.0.0-alpha.2.0 functionality.","bop-982-cannot-change-mke-4-password-using-mkectl#[BOP-982] Cannot change MKE 4 password using mkectl":"You cannot change the password for an existing MKE 4 deployment by running mkectl apply -f mke4.yaml --admin-password \u003cpassword\u003e, which is the expected behavior.\nWorkaround:\nUse kubectl to change the Password object:\nObtain the list of users:\n$ kubectl -n mke get passwords -o custom-columns=NAME:.metadata.name,EMAIL:.email Example output:\nNAME EMAIL mfsg22lozpzjzzeeeirsk admin Reveal the Password object for the target user.\n$ % km get password mfsg22lozpzjzzeeeirsk -oyaml Example output:\napiVersion: dex.coreos.com/v1 email: admin hash: JDJhJDEwJFA5RUppWmVJLkRCMVlqMWJqZk5rUk9RQ1oybFFpOUhXUFhnYmIxdUFPSkpHeGFDWUl1OTcy kind: Password metadata: creationTimestamp: \"2024-07-23T18:39:11Z\" generation: 1 name: mfsg22lozpzjzzeeeirsk namespace: mke resourceVersion: \"3558\" uid: 91a9e728-abfa-4daa-bdab-4c09cf888281 userID: 7668fdb9-a979-4645-b6cc-10985df77da6 username: admin Edit the hash field with the desired password hash."},"title":"Known issues"},"/docs/tutorials/":{"data":{"tutorials#Tutorials":"Tutorials Scenario 1: Terraform "},"title":"_index"},"/docs/tutorials/k0s-in-aws/terraform-scenario/":{"data":{"":"","clean-up-infrastructure#Clean up infrastructure":"To clean up and tear down infrastructure that is no longer needed, ensuring that all resources managed by Terraform are properly deleted, navigate to the Terraform folder and run:\nterraform destroy --auto-approve After successfully destroying the resources, Terraform will update the state file to reflect that the resources no longer exist.","create-virtual-machines-on-aws#Create virtual machines on AWS":"To create virtual machines on AWS using the example Terraform scripts:\nCopy the example Terraform folder to your local machine.\nCreate a terraform.tfvars file with content similar to:\ncluster_name = \"k0s-cluster\" controller_count = 1 worker_count = 1 cluster_flavor = \"m5.large\" region = \"us-east-1\" Run terraform init.\nRun terraform apply -auto-approve.\nRun terraform output --raw k0s_cluster \u003e VMs.yaml.\n‚ÑπÔ∏è To get detailed information on the virtual machines using the AWS CLI, run:\naws ec2 describe-instances --region $(grep \"region\" terraform.tfvars | awk -F' *= *' '{print $2}' | tr -d '\"') Alternatively, you can get a visual overview of the virtual machines at the AWS EC2 page by selecting the desired region from the dropdown menu in the top-right corner.","install-mke-on-k0s#Install MKE on k0s":" Generate a sample mke4.yaml file:\nmkectl init \u003e mke4.yaml Edit the hosts section in mke4.yaml using the values from the VMs.yaml file. Example configuration of the hosts section:\nhosts: - role: controller+worker ssh: address: 54.91.231.190 keyPath: \u003cpath_to_terraform_folder\u003e/aws_private.pem port: 22 user: ubuntu - role: worker ssh: address: 18.206.202.16 keyPath: \u003cpath_to_terraform_folder\u003e/aws_private.pem port: 22 user: ubuntu Edit the apiServer.externalAddress in the configuration file\nterraform output -raw lb_dns_name | { read lb; yq -i \".apiServer.externalAddress = \\\"$lb\\\"\" mke4.yaml; } If you do not have the yq tool installed, edit the mke4.yaml file manually setting apiServer.externalAddress to the output of the terraform output -raw lb_dns_name command.\nCreate the MKE cluster:\nmkectl apply -f mke4.yaml ‚ÑπÔ∏è Upon successful completion of the MKE 4 installation, a username and password will be automatically generated and displayed once for you to use.\nTo explicitly set a password value, run mkectl apply -f mke4.yaml --admin-password \u003cpassword\u003e .","prerequisites#Prerequisites":"In addition to the MKE dependencies, you need to do the following:\nInstall Terraform (required for creating VMs in AWS) Create an AWS account Set the environment variables for the AWS CLI: AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN "},"title":"Create a Kubernetes cluster in AWS using Terraform and install MKE"}}